{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Lab Sheet 1: PySpark Demo and Word Counting with Spark\n\nTo get you started, we'll walk you through a bits of PySpark code, and then we'll do the classic word count example, followd by some tasks for you to try.\n\n**Please run through the notebook cell by cell (using 'run' above or 'shift-return' on the keyboard).**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Part 1 - Demo: Apapche Spark API with PySpark\n\nBasically there are 3 API's available in Apache Spark - RDD (Resilient Distributed Datasets), DataFrame and Dataset. In this lab we will look at RDDs and Dataframes.\n\nFor more information on the Spark framework - visit (https://spark.apache.org)\nFor more information on the Pyspark API - visit (https://spark.apache.org/docs/latest/api/python/index.html)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1) Access to Spark\n\nThe PySpark notebook environment provides SparkSession object called `spark`. From there we can get the SparkContext, normally called `sc`, which we use to create RDDs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# predifined spark session\nprint(spark) \n# get the context\nsc = spark.sparkContext\nprint(sc)", 
            "metadata": {}
        }, 
        {
            "source": "### 2) RDD Creation\n\nThere are two ways to create RDDs. The first is to parallelise a Python object that exists in your driver process (i.e. this one). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "The second way to create an RDD is by referencing an external dataset such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat. This is what we will be using in this lab (further down).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Creat an RDD from a Python object in this process (the \"driver\").\n# The parallelize function  creating the \"numbers\" RDD\ndata = [1,2,3,4,5]\nfirstRDD = sc.parallelize(data)\nprint(firstRDD)", 
            "metadata": {}
        }, 
        {
            "source": "This RDD lives now on as many worker machines as are available and as are deemed useful by Spark.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3) RDD operations\nRDDs have two kinds of operations: *Transformations* and *Actions*.\n\n*Transformations* create a new RDD by applying a function to the items in the RDD. The function will only be applied when needed (\"*lazy* evaluation\").\n\n*Actions* produce some output from the data. An *Action* will trigger the execution of all *Transformations*.\n\nHere are some examples:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# lambda function: x -> x+3\nRDD2 = firstRDD.map(lambda x:x+3)  \nprint(RDD2)\n# nothing happened to far, as there is no action", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# \"count\" is an action and triggers the transformation   \na = RDD2.count() \nprint(a)", 
            "metadata": {}
        }, 
        {
            "source": "`collect` is an action that returns the values of the RDD in an Python array, back into this local driver process.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "a = RDD2.collect() \nprint(a)", 
            "metadata": {}
        }, 
        {
            "source": "Look here for more information about the functions provided by RDD: (https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 4) Dataframes \n\nDataframes are a more structured form of data storage than RDDs, similar to Pandas dataframes.  \n\nLet us see how to create and use dataframes. There are three ways of creating a dataframe\n    a) Loading data from an existing RDD.\n    b) It can be created form and external data source. For example, loading the data from JSON or CSV files.\n    c) Programmatically specifying schema.\n    \nHere is an example for option a.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from pyspark.sql import Row\n\nlist = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)]\nrdd = sc.parallelize(list)\npeopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\npeopleDF = sqlContext.createDataFrame(peopleRDD) \nprint(peopleDF)", 
            "metadata": {}
        }, 
        {
            "source": "### 5) Visualisation with Pixiedust\n\nThe pixiedust library provides a visual display of DataFrames, so we can use it to view the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import pixiedust\ndisplay(peopleDF)", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "name", 
                        "aggregation": "SUM", 
                        "handlerId": "barChart", 
                        "valueFields": "age", 
                        "rowCount": "500"
                    }
                }
            }
        }, 
        {
            "source": "## Part 2: Classic Word Count\n\nWe will now do the classic word count example for the MapReduce pattern.\n\nWe will apply it to the text of Sheakespeare's play *Hamlet*. For that you should have uploaded the file \"hamlet.txt\" into the data assets. If you don't have it, please follow the instructions in the *IBM project setup guide* on Moodle.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 6) Load the data\nFirst we need to load the text into an RDD (the second method of creating an RDD as mentioned above). \n\nFor this, go to the *Data* sidebar and select from *Insert to code* the option *Insert SparkSession Setup* below.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# insert file access code here ", 
            "metadata": {}
        }, 
        {
            "source": "You should have a variable `path_1` or similar, from which you can read the file into an RDD with `textFile`. The RDD then contains as items the lines of the text. `take(3)` then gives us the first 3 lines.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "lineRDD = sc.textFile(path_1)\nlineRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "### 7) Split lines into words\n\nIn order to count the words, we need to split the lines into words. We can do that using the `split` function of the Python String class to separate at each space. \n\nThe map function replaces each item with a new one, in this case our `lambda` returns an array of words (provided by `split(' ')`). However, we want to create one item per word, therefore we need to use a function called `flatMap` that creates a new RDD item for every item in the array returned by the lambda.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\nwordRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "Map the words to tuples of the form *(word, 1)*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "word1RDD = wordRDD.map(lambda x: (x, 1))\nword1RDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "### 8) Count by reducing\nFor Spark, the first part in each tuple is the 'key'. Now we can use reduceByKey() to add the 1s and get the number of occurences per word.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\nwordCountRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "### 9) Filtering and visualisation\n\nThere are many empty strings returned by the splitting. We can remove them by filtering.\nThen can take a shortcut and use a ready-made functions 'count by value', which does the same as we before.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\nword1RDD = wordFilteredRDD.map(lambda x: (x, 1))\nwordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\nwordCountRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "wordCountRows = wordCountRDD.map(lambda x: Row(word=x[0], count=int(x[1])))\nwordCountDF = sqlContext.createDataFrame(wordCountRows) \ndisplay(wordCountDF)", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "word", 
                        "aggregation": "SUM", 
                        "valueFields": "count", 
                        "handlerId": "barChart", 
                        "stretch": "true", 
                        "chartsize": "100", 
                        "rowCount": "50"
                    }
                }
            }
        }, 
        {
            "source": "## Part 3: Tasks for you to work on\n\nBased on the examples above, you can now write some code yourself. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Task 1) Better splitting \n\nAs you may have seen, currently our 'words' contain punctuation. A better was to split is using regular expressions  (Python's 're' package)(https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). `re.split('\\W+', 'my. test. string!')` does a better job. Try it out below by fixing the line that starts with '>>>'.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import re\n>>> wordRDD = lineRDD.flatMap(lambda x: ...) # apply re.split('\\W+', string) here\nwordFilteredRDD = wordRDD.filter(lambda x: len(x)>0) # do the filtering\nwordFilteredRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "## 2) Use lower case\n\nConvert all strings to lower case (using `.lower()` provided by the Python string class), so that 'Test' and 'test' count as the same. Package it into one a tuple of the form (word,1) in the same call.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": ">>> wordLowerRDD = wordFilteredRDD.map(lambda x: ... )\nwordLowerRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "wordCountRDD = wordLowerRDD.reduceByKey(lambda x,y: x+y) # we can now get better word count results\nwordCountRDD.take(3)", 
            "metadata": {}
        }, 
        {
            "source": "## 3) Filter rare words\n\nAdd a filtering step call remove all words with less than 5 occurrences. This can be useful to identify common topics in documents, where very rare words can be misleading. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# the trick here is to apply the lambda only to the second part of each item, i.e. x[1] \n>>> freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the times where the lambda returns true.\nfreqWordsRDD.take(3)\n", 
            "metadata": {}
        }, 
        {
            "source": "## 4) List only stopwords\n\nStopwords are frequent words that are not topic-specifc.  Stopwords can be useful for recognising the style of an author. Removing stopwords can be useful in regocnising the topic of a document. \n\nBelow is a small list of stopwords. Filter the tuples where the first part is a stopword.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "stopWordList = ['the','a','in','of','on','at','for','by','I','you','me'] \n>>> stopWordsRDD = freqWordsRDD.filter(lambda x:  ) # the 1st part of the tuple should be in the list ", 
            "metadata": {}
        }, 
        {
            "source": "There are only a few words, so we can see the vies results. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "output = stopWordsRDD.collect() \nfor (word, count) in output:\n    print(\"%s: %i\" % (word, count))", 
            "metadata": {}
        }, 
        {
            "source": "We can now visualise the stopword counts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "df2 = sqlContext.createDataFrame(output)\ndisplay(df2)", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "keyFields": "_1", 
                        "aggregation": "SUM", 
                        "handlerId": "barChart", 
                        "valueFields": "_2", 
                        "rowCount": "500"
                    }
                }
            }
        }, 
        {
            "source": "### 5) Reading\n\nRead chapter 1 of Lescovec et al (2014), \"Mining of Massive Datasets\", and work out the answers to exercise 1.2.1 on page 7 and 1.3.1 and 1.3.2 on page 15. If you have time, start reading chapter 2.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 6) Spark @home (recommended)\n\nInstall Spark on your own laptop or desktop, using the instructions provided on Moodle.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }
    }, 
    "nbformat_minor": 1
}