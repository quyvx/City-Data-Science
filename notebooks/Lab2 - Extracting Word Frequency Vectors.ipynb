{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "3.6.3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3", 
            "name": "python3"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Lab Sheet 2: Extracting Word Fequency Vectors with Spark\n\nThese tasks are for working in the lab session and during the week. We'll do a bit of word preprocessing in task 1) and in task 2) we'll load a number of files and will go through the processing steps to extract word frequencies. \n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Getting data from git\nThe cell below will create a local copy from our Github repository into the local filesystem. This only needs to be done once"
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!git clone https://github.com/tweyde/City-Data-Science.git", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The next cell will update the local copy of the repository. You should do this once a week, or when asked. "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "%cd City-Data-Science/\n!git pull https://github.com/tweyde/City-Data-Science.git\n%cd ..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 1) Word preparation\n\nDefine your own mapper function for removing the plural \u201cs\u201d at the end of words and turning them to lower case as a rough approximation towards stemming. \n\nUse the python def syntax [see here](https://docs.python.org/release/1.5.1p1/tut/functions.html) to define your own function stripFinalS(word) that takes as argument a word, and outputs the word in lower case without any possible trailing \u201cs\u201d.\n\nFor this task, you can treat strings as lists and apply \"list slicing\": <br>\n`lst[0:3] # the first three elements` <br>\n`lst[:-2] # all but the last two elements`\n\nYou need to check that the string is not empty (test `len(word)`) before accessing the letters in the string, otherwise you'll raise an exception.m"
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def stripFinalS( word ):\n#>>> add code here\n\nprint(stripFinalS('houses')) # for testing, should return 'house'", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Add your new function into the word count example below for testing, replacing the `lower()` method."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from operator import add\nimport re\n\nlinesRDD = sc.textFile(\"./City-Data-Science/library/hamlet.txt\") # read text as RDD\nwordsRDD = linesRDD.flatMap(lambda line: re.split('\\W+',line)) # split words, break lists\nwordsFilteredRDD = wordsRDD.filter(lambda word: len(word)>0)\n>>> words1RDD = wordsFilteredRDD.map(lambda word: (word.lower(),1)) # lower case, (w,1) pairs\nwordCountRDD = words1RDD.reduceByKey(add) # reduce and add up counts\nfreqWordsRDD = wordCountRDD.filter(lambda x:  x[1] >= 5 ) # remove rare words\noutput = freqWordsRDD.sortBy(lambda x: -x[1]).take(10) # collect 1o most frequent words\nfor (word, count) in output: # iterate over (w,c) pairs\n    print(\"%s: %i\" % (word, count)) #  \u2026 and print\n# this sohuld print the stopwords with their ", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# 2) Extracting word frequency vectors from text documents\n\nNow we start a new script, which reads in a whole directory with text files and extracts word frequency information.\n\nThis involves some tuple restructuing and list transformation. It is important to use meaningful variable names. Also it is helpful to use pen and paper (or a text editor) to write down the structures that you are intending to create. Keep in mind the final goal of getting a list of words and their frequencies for each file, i.e. (filename,[(w,c), ... , (w,c)]). \n\n\n## 2a) Load the files\nLoad all text files in the directory /data/student/bigdatastud/library on the server lewes using sc.wholeTextFiles <br>(see [http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles](http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles)). This will create an RDD with tuples of the structure (filepath,content), where content is the whole text from the file. "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "dirPath = \"./City-Data-Science/library/\"\n<<< add code to create an RDD with wholeTextFiles\nprint(\"partitions: \", fw_RDD.getNumPartitions()) # on IBM DSX we have 2 executors by default with one partition each\nprint(\"elements: \", fw_RDD.count())\n# this should print partitions:  2 and elements:  19", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2b) Split the RDD elements using flatMap to make the (filename, word) tuples the key.\n\nFor this define a function that takes a pair `(filename,content)` and output list of pairs `[(filename, word1), ...(filename, wordN)]`. You can get the words as usual by re.split(\u2019\\W+\u2019,x). \n\nUse list comprehensions (see http://www.pythonforbeginners.com/basics/list-comprehensions-in-python) to iterate through the word list in a for loop, and append the (filename,word) tuples to a new list.  \n\nBelow is a template, you need to fill in the that starts with `<<<`."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # unpack the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst =  # <<< now create a word list wLst be splitting c (the content)\n    for w in wLst: # iterate through the list\n         # <<< and append (f,w) to the fwList\n    return fwLst #return a list of (f,w) tuples \n    \nfw_RDD = fw_RDD.flatMap(splitFileWords)\nprint(fw_RDD.take(1))\n# should print something similar to this:\n# [('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n# 'The'),", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Question: \nLooking that the new elements, what might be problematic if we were using a large dataset and what could we do to prevent this from happening?"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now use filter to keep only the tuples with stopwords (remember, the words are now the 2nd element of the tuple)."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "stopwlst = ['the','a','in','of','on','at','for','by','I','you','me'] # stopword list\nfw_RDD2 = fw_RDD #<<< filter, keeping only stopwords as 2nd part of the tuples\nfw_RDD2.top(3) #", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "\n## 2c) Count the words and reorganise the tuples to count: ((filename,word), count)\n\nNow you can package the elements into tuples with 1s and use reduceByKey(add) to get the counts of the words per filename, similar to last week and in task 1 above."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "fw_1_RDD = fw_RDD2.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\nfw_c_RDD = fw_1_RDD.reduceByKey(add) #<<< count the words\nfw_c_RDD.top(3)\n#", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2d) Creating and concatenating lists\n\nAs a next step, map the `((filename,word),count)` eleemnts to `( filename, [ (word, count) ])` structure, i.e. rearange and wrap a list aournd the one tuple (just by writing squre backets). For this create a function `reGrpLst` to regroup and create a list. Check that the output has the intended structure."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def reGrpLst(fw_c): # we get a nested tuple\n    fw,c = fw_c # unpack the outer tuple\n>>> # unpack the inner tuple\n>>> # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) \nf_wcL_RDD.top(3)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Next we can concatenate the lists per filename using reduceByKey(). Write a lambda that cancatenates the lists per element.  Concatenation of lists in Python is done with '+', e.g.  `[1,2] + [3,4]` returns `[1,2,3,4]`."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": ">>> f_wcL2_RDD = f_wcL_RDD.reduceByKey(lambda wc1,wc2: ... ) #<<< create [(w,c), ... ,(w,c)] lists per file ", 
            "outputs": []
        }, 
        {
            "metadata": {
                "pixiedust": {
                    "displayParams": {}
                }
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "output = f_wcL2_RDD.collect() \nfor el in output[1:4]:\n    print(el)\n    print()\n# should show something like this:\n# ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/king_lear.txt', \n# [('of', 495), ('in', 280), ('at', 66), ('me', 228), ('I', 705), ('for', 130), ('on', 104), ('you', 412), ('a', 364), ('the', 746), ('by', 84)])", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 2e) Creating Hash Vectors\n\nIf we want to use all the words in each text, we need to reduce the dimensionality of the vectors. For this we use the 'Hashing Trick' shown in the lecture. \n\nStart by writing a function that takes a (word,count) list, and transforms it into vector of fixed size. For that you need to take the have value of each word module the size (`hash(word) % size`) and add up all counts that map here. "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def hashWcList(lst,size):\n    lst2 = [0] * size;\n    for (w,c) in lst:\n>>>        lst2[...] ... # determine the position with hash(w)%size and add c there\n    return lst2\n        \nhashWcList([('this',23),('is',12),('a',34),('little',13),('test',24)],5) # for testing\n#output should look like this: [36, 0, 36, 0, 34]", 
            "outputs": []
        }, 
        {
            "metadata": {
                "pixiedust": {
                    "displayParams": {}
                }
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "f_hv_RDD = wordCountRDD.map(lambda f_wcl: (f_wcl[0],hashWcList(f_wcl[1],10)))\n\nfor el in output[1:4]:\n    print(el)\n    print()\n# now we can display a 10-dimensional vector for every text file ", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}