{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Lab Sheet 3 - Solution: Extracting Word Frequency Vectors with Spark\n\nThese tasks are for working in the lab session and during the week. We will use the same data as last week (19 files in './City-Data-Science/library/') and use some more RDD functions. We will apply two different approaches to create and use fixed size vectors.\n\nFirst update the repo.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 10, 
            "metadata": {}, 
            "source": "!git clone https://github.com/tweyde/City-Data-Science.git\n%cd City-Data-Science/\n!git pull https://github.com/tweyde/City-Data-Science.git\n%cd ..", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "fatal: destination path 'City-Data-Science' already exists and is not an empty directory.\n/Users/tweyde/Documents/CityUni/teaching/2017-18/Big Data/labs/lab3/City-Data-Science\nFrom https://github.com/tweyde/City-Data-Science\n * branch            HEAD       -> FETCH_HEAD\nAlready up-to-date.\n/Users/tweyde/Documents/CityUni/teaching/2017-18/Big Data/labs/lab3\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "Here is code from last week that we run first, and then extend. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 11, 
            "metadata": {}, 
            "source": "import re \n\ndef stripFinalS( word ):\n    word = word.lower() # lower case\n    if len(word) >0 and word[-1] == 's': # check for final letter\n        return word[:-1]\n    else:\n        return word\n    \ndef splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # split the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n    for w in wLst : # iterate through the list\n        fwLst.append((f,stripFinalS(w))) # and append (f,w) to the \n    return fwLst #return a list of (f,w) tuples \n\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n\ndirPath = './City-Data-Science/library/' #  path\nft_RDD = sc.wholeTextFiles(dirPath) #<<< add code to create an RDD with wholeTextFiles\nfnt_RDD = ft_RDD.map(lambda ft: (re.split('[/\\.]',ft[0])[-2],ft[1])) # just take filename, \n                                                # drop path and extension for readability\nfw_RDD1 = fnt_RDD.flatMap(splitFileWords)\nfw_RDD = fw_RDD1.filter(lambda fw: len(fw[1])>0 and fw[1] not in ['project','gutenberg', 'ebook'])  \nfw_RDD.take(3)", 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('emma', 'the'), ('emma', 'of'), ('emma', 'emma')]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 1) Warm-up\nLet's start with a few small tasks, to become more fluent with RDDs and lambda expressions.\n\na) Count the number of documents.\nb) Determine the number distinct words in total (the vocabulary size) using RDD.distinct(). This involves removing the fs from the (f,w) pairs and geting getting the RDD size (with RDD.count()). \nc) Get the number of words (including repeated ones) per book. \nd) Determine the number of distinct words per book. This involves determining the disting (f,w) pairs, geting a list of words per file, and getting the list size.\ne) Count the average number of occurences per word per file (words/vocabulary). Use RDD.join() to get both numbers into one RDD. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 12, 
            "metadata": {}, 
            "source": "# a) Library size\nprint(\"Number of documents: \",ft_RDD.count())\n\n# b) Vocabulary size\nw_RDD = fw_RDD.map(lambda fw: fw[1])\nw_RDDu = w_RDD.distinct()\nprint('Total vocabulary size: ',w_RDDu.count())\n# this can also be programmed in short as follows:\nprint('Total vocabulary size (v2): ',fw_RDD.values().distinct().count())\n\n# c) words per book\nf1_RDD = fw_RDD.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (w,1)\nfc_RDD = f1_RDD.reduceByKey(add)\nprint('Words per book: ',fc_RDD.take(3))\n# or alternatively (both versions should produce the same output):\nprint('Words per book (v2): ',fw_RDD.countByKey())\n\n# d) Vocabulary per book\nfrom operator import add\nfw_RDDu = fw_RDD.distinct() # get unique (f,w) pairs - i.e. evey word only once per file. I use postfix u to mark 'unique'\nf1_RDDu = fw_RDDu.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (w,[f])\nfcu_RDD = f1_RDDu.reduceByKey(add)\nprint('Vocabulary per book: ',fcu_RDD.take(3))\n# or, again, shorter:\nprint('Vocabulary per book (v2): ',fw_RDD.distinct().countByKey())\n\n# e) Average occurences of words per book (i.e. words/vocab per book)\nf_wv_RDD = fc_RDD.join(fcu_RDD) # join the two RDDs to get (f,(w,v)) tuples\nprint(f_wv_RDD.take(3)) \nf_awo_RDD = f_wv_RDD.map(lambda f_wv: (f_wv[0],f_wv[1][0]/f_wv[1][1])) # this is the tricky part. \n# Resolve nested tuples in the lambda to get (filename,words/vocab) tuples\nprint('Average word occurences: ',f_awo_RDD.take(3))\n# should look like this [('henry_V', 6.237027812370278), ('king_lear', 7.815661103979461), ('lady_susan', 8.531763947113834)]", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Number of documents:  18\nTotal vocabulary size:  23368\nTotal vocabulary size (v2):  23368\nWords per book:  [('henry_V', 29900), ('king_lear', 30119), ('lady_susan', 26109)]\nWords per book (v2):  defaultdict(<class 'int'>, {'emma': 163979, 'hamlet': 34518, 'henry_V': 29900, 'julius_cesar': 22959, 'king_lear': 30119, 'lady_susan': 26109, 'macbeth': 20810, 'mansfield_park': 163509, 'merchant_of_venice': 24772, 'midsummer': 19814, 'northanger_abbey': 77582, 'othello': 30617, 'persuasion': 83680, 'prideandpredjudice': 125212, 'richard_III': 33785, 'romeo_and_juliet': 29538, 'senseandsensibility': 123066, 'tempest': 27891})\nVocabulary per book:  [('henry_V', 4815), ('king_lear', 3892), ('lady_susan', 3097)]\nVocabulary per book (v2):  defaultdict(<class 'int'>, {'emma': 6667, 'hamlet': 4447, 'henry_V': 4815, 'julius_cesar': 2834, 'king_lear': 3892, 'lady_susan': 3097, 'macbeth': 3654, 'mansfield_park': 7492, 'merchant_of_venice': 3686, 'midsummer': 3448, 'northanger_abbey': 5510, 'othello': 4077, 'persuasion': 5268, 'prideandpredjudice': 6181, 'richard_III': 3824, 'romeo_and_juliet': 3722, 'senseandsensibility': 6073, 'tempest': 4727})\n[('henry_V', (29900, 4815)), ('king_lear', (30119, 3892)), ('lady_susan', (26109, 3097))]\nAverage word occurences:  [('henry_V', 6.209761163032191), ('king_lear', 7.738694758478931), ('lady_susan', 8.430416532127866)]\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "\n## 2) Fixed vectors: Reduced vocabulary approach\n\nThe first task in this lab is to use a reduced vocabulary, only the stopwords from a list, to make sure that we have a fixed size vector. This is a common approach in stylometry. The problem is that some stopwords might not appear in some documents. We will deal with that by creating an RDD with ((f,w),0) tuples that we then merge with the ((f,w),count) RDD. \n\nStart by running the code above, then you can add 1s and use reduceByKey(add) like last week to get the counts of the words per filename.\n\nThen, please make sure that all stopwords are present by creating a new RDD that contains the keys of the fw_RDD, i.e. the filenames, using the keys() method of class RDD. Then you can use flatMap to create a [((filname,stopword),0), ...] list, using a list comprehension. The 0s should not be 1s, as we don't want add to add extra counts.\nThe RDD with ((filename,stopword),0) tuples can then be merged with fw_RDD2 using union(). Then you can count as normal.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 13, 
            "metadata": {}, 
            "source": "from operator import add\n\nstopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\nfw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) # filter, keeping only stopwords\n\n#fw_RDD.keys().collect()\nfsw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f,sw),0) for sw in stopwlst])\nprint(fsw_0_RDD.take(3))\n\nfw_1_RDD = fw_RDD2.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\nprint(fw_1_RDD.take(3))\n\nfw_10_RDD = fw_1_RDD.union(fsw_0_RDD)\nprint(fw_10_RDD.take(3))\n\nfw_c_RDD = fw_10_RDD.reduceByKey(add) #<<< count the words\nprint(fw_c_RDD.take(3))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[(('emma', 'the'), 0), (('emma', 'a'), 0), (('emma', 'in'), 0)]\n[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n[(('emma', 'the'), 5380), (('emma', 'by'), 591), (('emma', 'you'), 2068)]\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 3) Creating sorted lists\n\nAs a next step, map the `((filename,word),count)` to `( filename, [ (word, count) ])` using the function `reGrpLst` to regroup and create a list. \n\nThen sort the [(word,count),...] lists in the values (i.e. 2nd part of the tuple) with the the words as keys. Have a [look at the Python docs](https://docs.python.org/3.5/library/functions.html?highlight=sorted#sorted) for how to do this. Hint: use a lambda that extracts the words as the key, e.g. `sorted(f_wdL[1], key = lambda wc: ... )`.   ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 14, 
            "metadata": {}, 
            "source": "def reGrpLst(fw_c): # we get a nested tuple\n    fw,c = fw_c     # split the outer tuple\n    f,w = fw        # split the inner tuple\n    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n\nfw_RDD\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) \nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file \nf_wcLsort_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], sorted(f_wcL[1],key=lambda x: x[0]))) #<<< sort the lists\nprint(f_wcLsort_RDD.take(3))\nf_wVec_RDD = f_wcLsort_RDD.map(lambda f_wc: (f_wc[0],[float(c) for (w,c) in f_wc[1]])) #<<< remove the words and convert the numbers to floats\nf_wVec_RDD.take(3)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[('lady_susan', [('a', 611), ('at', 161), ('by', 152), ('for', 262), ('i', 1106), ('in', 402), ('me', 200), ('of', 787), ('on', 140), ('the', 784), ('you', 353)]), ('macbeth', [('a', 395), ('at', 64), ('by', 74), ('for', 142), ('i', 581), ('in', 227), ('me', 119), ('of', 427), ('on', 76), ('the', 765), ('you', 272)]), ('merchant_of_venice', [('a', 646), ('at', 75), ('by', 131), ('for', 254), ('i', 976), ('in', 319), ('me', 260), ('of', 535), ('on', 81), ('the', 938), ('you', 507)])]\n", 
                    "name": "stdout"
                }, 
                {
                    "output_type": "execute_result", 
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('lady_susan',\n  [611.0,\n   161.0,\n   152.0,\n   262.0,\n   1106.0,\n   402.0,\n   200.0,\n   787.0,\n   140.0,\n   784.0,\n   353.0]),\n ('macbeth',\n  [395.0, 64.0, 74.0, 142.0, 581.0, 227.0, 119.0, 427.0, 76.0, 765.0, 272.0]),\n ('merchant_of_venice',\n  [646.0, 75.0, 131.0, 254.0, 976.0, 319.0, 260.0, 535.0, 81.0, 938.0, 507.0])]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 4) Clustering\n\nNow we have feature vectors of fixed size, we can use KMeans as provided by Spark.\n\nThe files in our library are by two authors. After clustering, check if the cluters reflect authorship:\n\nWILLIAM SHAKESPEARE: \nmerchant_of_venice, \nrichard_III, \nmidsummer,\ntempest,\nromeoandjuliet,\nothello,\nhenry_V,\nmacbeth,\nking_lear,\njulius_cesar,\nhamlet\n\nJANE AUSTEN\nmansfield_park,\nemma,\nnorthanger_abbey,\nlady_susan,\npersuasion,\nprideandpredjudice,\nsenseandsensibility", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 15, 
            "metadata": {}, 
            "source": "\nfrom math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\n#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\nwVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n#print(wVec_RDD.collect())\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusterModel.centers[clusterModel.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "('lady_susan', 1)\n('macbeth', 1)\n('merchant_of_venice', 1)\n('othello', 1)\n('persuasion', 0)\n('emma', 0)\n('julius_cesar', 1)\n('mansfield_park', 0)\n('richard_III', 1)\n('tempest', 1)\n('henry_V', 1)\n('king_lear', 1)\n('midsummer', 1)\n('northanger_abbey', 0)\n('prideandpredjudice', 0)\n('romeo_and_juliet', 1)\n('senseandsensibility', 0)\n('hamlet', 1)\nWithin Set Sum of Squared Error = 15118.000047182637\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 5) Alternative approach: feature hashing\n\nInstead of the previous appraoch, we now use feature hashing, as done last week.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 18, 
            "metadata": {}, 
            "source": "def hashing_vectorizer(word_count_list, N):\n     v = [0] * N  # create fixed size vector of 0s\n     for word_count in word_count_list: \u000b         word,count = word_count \t# unpack tuple\n         h = hash(word) # get hash value\n         v[h % N] = v[h % N] + count # add count\n     return v \t# return hashed word vector\n\nfrom operator import add\n\nN = 10\n\n# we use fw_RDD from the beginning with all the words, not just stopwords\nfw_1_RDD = fw_RDD.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\nfw_c_RDD = fw_1_RDD.reduceByKey(add) #as above\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) #as above\nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file \nf_wVec_RDD = f_wcL2_RDD.map(lambda f_wc: (f_wc[0],hashing_vectorizer(f_wc[1],N)))\nprint(f_wVec_RDD.take(3))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[('henry_V', [3179, 3045, 2431, 2814, 2946, 3252, 2144, 4707, 1821, 3561]), ('king_lear', [3619, 3106, 2258, 3538, 3393, 2844, 2236, 4138, 2111, 2876]), ('lady_susan', [3448, 2639, 1874, 2737, 2847, 2894, 1994, 3851, 1158, 2667])]\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 19, 
            "metadata": {}, 
            "source": "from math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\n#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\nwVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n#print(wVec_RDD.collect())\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusterModel.centers[clusterModel.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "('henry_V', 1)\n('king_lear', 1)\n('lady_susan', 1)\n('macbeth', 1)\n('merchant_of_venice', 1)\n('midsummer', 1)\n('northanger_abbey', 1)\n('othello', 1)\n('persuasion', 1)\n('prideandpredjudice', 0)\n('romeo_and_juliet', 1)\n('senseandsensibility', 0)\n('emma', 0)\n('hamlet', 1)\n('julius_cesar', 1)\n('mansfield_park', 0)\n('richard_III', 1)\n('tempest', 1)\nWithin Set Sum of Squared Error = 90883.15633407317\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 6) Neutralising document length: Normalised vectors\n\n'Lady Susan' ends up reliably in the wrong cluster. A possible explanation is that it is shorter than the other Austen works. Try normalising the word counts, i.e. by dividing by their sum. That takes away the effect of length. What is the effect on the clustering?\n    \nYou can use a list comprehension for the normalisation.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 21, 
            "metadata": {}, 
            "source": "nwVec_RDD = wVec_RDD.map(lambda v: ([c/sum(v) for c in v])) \n# provide a list compresention that normalises the values by the \nprint(\"Normalised vectors: \",nwVec_RDD.take(3))\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(nwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n\n", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Normalised vectors:  [[0.10632107023411372, 0.10183946488294314, 0.08130434782608696, 0.09411371237458194, 0.09852842809364548, 0.10876254180602006, 0.0717056856187291, 0.1574247491638796, 0.06090301003344482, 0.11909698996655518], [0.12015671171021615, 0.10312427371426675, 0.07496928848899366, 0.11746737939506624, 0.1126531425346127, 0.09442544573193001, 0.07423885255154554, 0.13738835950728776, 0.07008864836149939, 0.09548789800458182], [0.1320617411620514, 0.1010762572293079, 0.07177601593320311, 0.10482975219273048, 0.10904285878432725, 0.1108430043280095, 0.07637213221494504, 0.14749703167490139, 0.044352522118809606, 0.10214868436171436]]\n('henry_V', 0)\n('king_lear', 0)\n('lady_susan', 0)\n('macbeth', 0)\n('merchant_of_venice', 0)\n('midsummer', 0)\n('northanger_abbey', 0)\n('othello', 0)\n('persuasion', 0)\n('prideandpredjudice', 0)\n('romeo_and_juliet', 0)\n('senseandsensibility', 0)\n('emma', 0)\n('hamlet', 0)\n('julius_cesar', 0)\n('mansfield_park', 0)\n('richard_III', 0)\n('tempest', 0)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 7) Building an index\n\nStarting from the fw_RDD we now start building the index and calculating the IDF values. Since we have the TF values alread, we only need to keep the unique filenames per word using [RDD.distinct()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.distinct).  \nThen we create a list of filenames. The length of the list is the document frequency DF per word.\nFrom the DF value we can calculate the IDF value as log(18/DF) ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 22, 
            "metadata": {}, 
            "source": "from operator import add\nfrom math import log\n\nfwu_RDD = fw_RDD.distinct() # get unique file/word pairs\nwfl_RDD = fwu_RDD.map(lambda fw: (fw[1],[fw[0]])) # create (w,[f]) tuples \nwfL_RDD = wfl_RDD.reduceByKey(add) # concatenate the lists with 'add'\nprint(wfL_RDD.take(3))\n\nwdf_RDD = wfL_RDD.map(lambda wfl: (wfl[0],len(wfl[1]))) # get the DF replacing the file list with its lenght\nprint(\"DF: \",wdf_RDD.take(3))\nwidf_RDD = wdf_RDD.map(lambda wdf: (wdf[0],log(18/wdf[1]))) # get he IDF by replacing DF with log(19/DF)\nprint(\"IDF: \",widf_RDD.take(3))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[('of', ['henry_V', 'king_lear', 'lady_susan', 'macbeth', 'merchant_of_venice', 'midsummer', 'northanger_abbey', 'othello', 'persuasion', 'prideandpredjudice', 'romeo_and_juliet', 'senseandsensibility', 'emma', 'hamlet', 'julius_cesar', 'mansfield_park', 'richard_III', 'tempest']), ('shakespeare', ['henry_V', 'king_lear', 'macbeth', 'merchant_of_venice', 'midsummer', 'northanger_abbey', 'othello', 'romeo_and_juliet', 'senseandsensibility', 'emma', 'hamlet', 'julius_cesar', 'mansfield_park', 'richard_III', 'tempest']), ('henry', ['henry_V', 'merchant_of_venice', 'midsummer', 'northanger_abbey', 'persuasion', 'senseandsensibility', 'emma', 'mansfield_park', 'richard_III'])]\nDF:  [('of', 18), ('shakespeare', 15), ('henry', 9)]\nIDF:  [('of', 0.0), ('shakespeare', 0.1823215567939546), ('henry', 0.6931471805599453)]\n", 
                    "name": "stdout"
                }
            ]
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python with Pixiedust (Spark 2.2)", 
            "name": "pythonwithpixiedustspark22"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.6.3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}