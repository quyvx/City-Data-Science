{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Lab Sheet 2 - Solutions: Extracting Word Frequency Vectors with Spark\n\nThese tasks are for working in the lab session and during the week. We'll do a bit of word preprocessing in task 1) and in task 2) we'll load a number of files and will go through the processing steps to extract word frequencies. \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 1) Word preparation\n\nDefine your own mapper function for removing the plural \u201cs\u201d at the end of words and turning them to lower case as a rough approximation towards stemming. \n\nUse the python def syntax [see here](https://docs.python.org/release/1.5.1p1/tut/functions.html) to define your own function stripFinalS(word) that takes as argument a word, and outputs the word in lower case without any possible trailing \u201cs\u201d.\n\nFor this task, you can treat strings as lists and apply \"list slicing\": <br>\n`lst[0:3] # the first three elements` <br>\n`lst[:-2] # all but the last two elements`\n\nYou need to check that the string is not empty (test `len(word)`) before accessing the letters in the string, otherwise you'll raise an exception.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 18, 
            "metadata": {}, 
            "source": "def stripFinalS( word ):\n#>>> add code here\n    wordl = word.lower() # lower case\n    if len(wordl) >1 and wordl[-1] == 's': # check for final letter\n        return wordl[:-1] # remove final letter\n    else:\n        return wordl; # return unchanged\n    \nprint(stripFinalS('Houses')) # for testing, should return 'house'", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "house\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comments\n- Slicing the Python array: `word[-1]` gives the last letter, `word[:-1]` gives all letters from the beginf to before the last (exclusive). For more information, look [here](https://docs.python.org/3.6/tutorial/datastructures.html#more-on-lists) in section 3.1.2 for 'slice' \n- Checking that the word string is not empty (`len(word)>0`) is useful. I filter empty string out in the code below, but there is no guarantee that that happens in other contexts.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Add your new function into the word count example below for testing, replacing the `lower()` method.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 19, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {}
                }
            }, 
            "source": "from operator import add\nimport re\n\nlinesRDD = sc.textFile(\"./City-Data-Science/library/hamlet.txt\") # read text as RDD\nwordsRDD = linesRDD.flatMap(lambda line: re.split('\\W+',line)) # split words, break lists\nwordsFilteredRDD = wordsRDD.filter(lambda word: len(word)>0) # filter empty words out\n#>>>\nwords1RDD = wordsFilteredRDD.map(lambda word: (stripFinalS(word),1)) # lower case, (w,1) pairs\nwordCountRDD = words1RDD.reduceByKey(add) # reduce and add up counts\nfreqWordsRDD = wordCountRDD.filter(lambda x:  x[1] >= 3 ) # remove rare words\noutput = freqWordsRDD.sortBy(lambda x: -x[1]).take(10) # collect 1o most frequent words\nfor (word, count) in output: # iterate over (w,c) pairs\n    print(\"%s: %i\" % (word, count)) #  \u2026 and print", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "the: 1218\ni: 1021\nand: 1019\nto: 834\na: 817\nof: 733\nyou: 610\nmy: 516\nin: 464\nit: 442\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comment\nThis task is very similar to last week, no surprises ... ;-)\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "# 2) Extracting word frequency vectors from text documents\n\nNow we start a new script, which reads in a whole directory with text files and extracts word frequency information.\n\nThis involves some tuple restructuing and list transformation. It is important to use meaningful variable names. Also it is helpful to use pen and paper (or a text editor) to write down the structures that you are intending to create. Keep in mind the final goal of getting a list of words and their frequencies for each file, i.e. (filename,[(w,c), ... , (w,c)]). \n\n\n## 2a) Load the files\nLoad all text files in the directory /data/student/bigdatastud/library on the server lewes using sc.wholeTextFiles <br>(see [http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles](http://spark.apache.org/docs/2.0.0/api/python/pyspark.html#pyspark.SparkContext.wholeTextFiles)). This will create an RDD with tuples of the structure (filepath,content), where content is the whole text from the file. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 20, 
            "metadata": {}, 
            "source": "dirPath = \"./City-Data-Science/library/\"\nfw_RDD = sc.wholeTextFiles(dirPath) #<<< add code to create an RDD with wholeTextFiles\nprint(\"partitions: \", fw_RDD.getNumPartitions()) # on IBM DSX we have 2 executors by default with one partition each\nprint(\"elements: \", fw_RDD.count())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "partitions:  2\nelements:  19\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comment\nStraightforward, just read from the given path. Reads all the files into (path,content) pairs.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2b) Split the RDD elements using flatMap to get (filename, word) elements.\n\nFor this, define a function that takes a pair `(filename,content)` and output list of pairs `[(filename, word1), ...(filename, wordN)]`. You can get the words as usual by re.split(\u2019\\W+\u2019,x). \n\nUse list comprehensions (see http://www.pythonforbeginners.com/basics/list-comprehensions-in-python) to iterate through the word list in a for loop, and append the (filename,word) tuples to a new list.  \n\nBelow is a template, you need to fill in the that starts with `<<<`.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 21, 
            "metadata": {}, 
            "source": "def splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # split the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n    for w in wLst: # iterate through the list\n        if len(w) >0: \n            fwLst.append((f,w)) # <<< and append (f,w) to the \n    return fwLst #return a list of (f,w) tuples \n    \nfw_RDD = fw_RDD.flatMap(splitFileWords)\nfw_RDD.take(3)", 
            "outputs": [
                {
                    "execution_count": 21, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n  'The'),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n  'Project'),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt',\n  'Gutenberg')]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comments\n- Building the list `fwLst` is the main new concept here. \n- Creating tuples with brackets is a technique that is frequently used.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now use filter to keep only the tuples with stopwords (remember, the words are now the 2nd element of the tuple).", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 22, 
            "metadata": {}, 
            "source": "stopwlst = ['the','a','in','of','on','at','for','by','I','you','me'] # stopword list\nfw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) #<<< filter, keeping only stopwords\nfw_RDD2.top(3)", 
            "outputs": [
                {
                    "execution_count": 22, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  'you'),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  'you'),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  'you')]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comment\n- With RDD.filter(), it is important to returen a boolean.\n- Important: `filter` keeps only elements, where the provided function (here as a lambda) returns `true`.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "\n## 2c) Count the words and reorganise the tuples to count: ((filename,word), count)\n\nNow you can package the elements into tuples with 1s and use reduceByKey(add) to get the counts of the words per filename, similar to last week and in task 1 above.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 23, 
            "metadata": {}, 
            "source": "fw_1_RDD = fw_RDD2.map(lambda x: (x,1))  #<<< change (f,w) to ((f,w),1)\nfw_c_RDD = fw_1_RDD.reduceByKey(add) #<<< count the words\nfw_c_RDD.top(3)\n# the printed elements should look similar to this:\n# (('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n#   'you'),\n#  260)", 
            "outputs": [
                {
                    "execution_count": 23, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n   'you'),\n  260),\n (('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n   'the'),\n  695),\n (('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n   'on'),\n  85)]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comment \nThis example follows the word count example, with the difference of keeping the filename in addition to the word.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2d) Creating and concatenating lists\n\nAs a next step, map the `((filename,word),count)` eleemnts to `( filename, [ (word, count) ])` structure, i.e. rearange and wrap a list aournd the one tuple (just by writing squre backets). For this create a function `reGrpLst` to regroup and create a list. Check that the output has the intended structure.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 24, 
            "metadata": {}, 
            "source": "def reGrpLst(fw_c): # we get a nested tuple\n    fw,c = fw_c\n    f,w = fw\n    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) \nf_wcL_RDD.top(3)", 
            "outputs": [
                {
                    "execution_count": 24, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  [('you', 260)]),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  [('the', 695)]),\n ('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/tempest.txt',\n  [('on', 85)])]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "Next we can concatenate the lists per filename using reduceByKey(). Write a lambda that cancatenates the lists per element.  Concatenation of lists in Python is done with '+', e.g.  `[1,2] + [3,4]` returns `[1,2,3,4]`.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comments\nHere we have a new technique: creating lists instead of tuples (using `[]` instead of `()`). The approach is similar to that of word counting, but adding lists (with `+` or `add`) means concatenating them, so that we produce a long list.  ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 25, 
            "metadata": {}, 
            "source": "f_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #<<< create [(w,c), ... ,(w,c)] lists per file ", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 26, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {}
                }
            }, 
            "source": "output = f_wcL2_RDD.collect() \nfor el in output[1:4]:\n    print(el)\n    print()", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/king_lear.txt', [('me', 228), ('I', 705), ('for', 130), ('on', 104), ('you', 412), ('a', 364), ('the', 746), ('by', 84), ('of', 495), ('in', 280), ('at', 66)])\n\n('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/othello.txt', [('by', 106), ('me', 278), ('a', 432), ('on', 123), ('I', 874), ('you', 490), ('the', 699), ('for', 189), ('of', 503), ('at', 70), ('in', 322)])\n\n('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/emma.txt', [('on', 688), ('by', 576), ('me', 574), ('for', 1342), ('the', 5006), ('a', 3062), ('I', 3200), ('you', 1743), ('at', 1013), ('in', 2172), ('of', 4389)])\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Extra task: Creating Hash Vectors\n\nIf we want to compare the word-counts for different files, and in particular if we want to use not just the stopwords, we need to bring them to one dimensionality as vectors. For this we use the 'Hashing Trick' shown in the lecture. \n\nStart by writing a function that takes a (word,count) list, and transforms it into vector of fixed size. For that you need to take the hash value of each word modulo the size (`hash(word) % size`) and add up all counts of words that map here. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 27, 
            "metadata": {}, 
            "source": "def hashWcList(lst,size):\n    lst2 = [0] * size; # create a vector of the needed size filled with '0's\n    for (w,c) in lst: # for every (word,count) pair in the given list\n        lst2[hash(w)%size] += c # add the count to the position where the word gets hashed to. \n    return lst2 # return the new list, containing only numbers\n        \nhashWcList([('this',23),('is',12),('a',34),('little',13),('test',24)],5) # for testing\n#output should look like this: [36, 0, 36, 0, 34]", 
            "outputs": [
                {
                    "execution_count": 27, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[36, 0, 36, 0, 34]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Comments\nThis method gives us a single vector that represents every text document as a compact vector of fixed dimension. \nVector like this can be used for findning documents in databases, grouping them by similarity, studying writing styles etc. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 28, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart"
                    }
                }
            }, 
            "source": "f_hv_RDD = f_wcL2_RDD.map(lambda f_wcl: (f_wcl[0],hashWcList(f_wcl[1],10)))\noutput = f_hv_RDD.collect()\nfor el in output[1:4]:\n    print(el)\n    print()\n# now we can display a hashed vector for every text file ", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/king_lear.txt', [561, 496, 280, 705, 0, 332, 0, 876, 0, 364])\n\n('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/prideandpredjudice.txt', [4464, 1824, 1850, 2066, 0, 1122, 0, 5280, 0, 1962])\n\n('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/library/emma.txt', [5402, 2319, 2172, 3200, 0, 1262, 0, 6348, 0, 3062])\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Extra Demos: exracting file names with regular expressios and creating a DataFrame to show with Pixiedust\nWe first use splittig with Regular Expressions to get just the filename without the path and the exentsion. \nThen we convert the RDD into a data frame ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 29, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            }, 
            "source": "fn_hv_RDD = f_hv_RDD.map(lambda x: (re.split('[/\\.]',x[0])[-2],x[1])) \nfn_hv_RDD.take(3)", 
            "outputs": [
                {
                    "execution_count": 29, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('julius_cesar', [458, 460, 214, 531, 0, 272, 0, 695, 0, 241]),\n ('king_lear', [561, 496, 280, 705, 0, 332, 0, 876, 0, 364]),\n ('othello', [573, 596, 322, 874, 0, 401, 0, 888, 0, 432])]"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            }, 
            "source": "import pixiedust\ndisplay(fn_hv_RDD.toDF(['title', 'vec']))", 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n            \n        </div>\n    <div id=\"chartFigure83dd298f\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n            <style type=\"text/css\" class=\"pd_save\">\n    .df-table-wrapper .panel-heading {\n      border-radius: 0;\n      padding: 0px;\n    }\n    .df-table-wrapper .panel-heading:hover {\n      border-color: #008571;\n    }\n    .df-table-wrapper .panel-title a {\n      background-color: #f9f9fb;\n      color: #333333;\n      display: block;\n      outline: none;\n      padding: 10px 15px;\n      text-decoration: none;\n    }\n    .df-table-wrapper .panel-title a:hover {\n      background-color: #337ab7;\n      border-color: #2e6da4;\n      color: #ffffff;\n      display: block;\n      padding: 10px 15px;\n      text-decoration: none;\n    }\n    .df-table-wrapper {\n      font-size: small;\n      font-weight: 300;\n      letter-spacing: 0.5px;\n      line-height: normal;\n      height: inherit;\n      overflow: auto;\n    }\n    .df-table-search {\n      margin: 0 0 20px 0;\n    }\n    .df-table-search-count {\n      display: inline-block;\n      margin: 0 0 20px 0;\n    }\n    .df-table-container {\n      max-height: 50vh;\n      max-width: 100%;\n      overflow-x: auto;\n      position: relative;\n    }\n    .df-table-wrapper table {\n      border: 0 none #ffffff;\n      border-collapse: collapse;\n      margin: 0;\n      min-width: 100%;\n      padding: 0;\n      table-layout: fixed;\n      height: inherit;\n      overflow: auto;\n    }\n    .df-table-wrapper tr.hidden {\n      display: none;\n    }\n    .df-table-wrapper tr:nth-child(even) {\n      background-color: #f9f9fb;\n    }\n    .df-table-wrapper tr.even {\n      background-color: #f9f9fb;\n    }\n    .df-table-wrapper tr.odd {\n      background-color: #ffffff;\n    }\n    .df-table-wrapper td + td {\n      border-left: 1px solid #e0e0e0;\n    }\n  \n    .df-table-wrapper thead,\n    .fixed-header {\n      font-weight: 600;\n    }\n    .df-table-wrapper tr,\n    .fixed-row {\n      border: 0 none #ffffff;\n      margin: 0;\n      padding: 0;\n    }\n    .df-table-wrapper th,\n    .df-table-wrapper td,\n    .fixed-cell {\n      border: 0 none #ffffff;\n      margin: 0;\n      min-width: 50px;\n      padding: 5px 20px 5px 10px;\n      text-align: left;\n      word-wrap: break-word;\n    }\n    .df-table-wrapper th {\n      padding-bottom: 0;\n      padding-top: 0;\n    }\n    .df-table-wrapper th div {\n      max-height: 1px;\n      visibility: hidden;\n    }\n  \n    .df-schema-field {\n      margin-left: 10px;\n    }\n  \n    .fixed-header-container {\n      overflow: hidden;\n      position: relative;\n    }\n    .fixed-header {\n      border-bottom: 2px solid #000;\n      display: table;\n      position: relative;\n    }\n    .fixed-row {\n      display: table-row;\n    }\n    .fixed-cell {\n      display: table-cell;\n    }\n  </style>\n  \n  \n  <div class=\"df-table-wrapper df-table-wrapper-83dd298f panel-group pd_save\">\n    <!-- dataframe schema -->\n    \n    <div class=\"panel panel-default\">\n      <div class=\"panel-heading\">\n        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n          <a data-toggle=\"collapse\" href=\"#df-schema-83dd298f\" data-parent=\"#df-table-wrapper-83dd298f\">Schema</a>\n        </h4>\n      </div>\n      <div id=\"df-schema-83dd298f\" class=\"panel-collapse collapse\">\n        <div class=\"panel-body\" style=\"font-family: monospace;\">\n          <div class=\"df-schema-fields\">\n            <div>Field types:</div>\n            \n              <div class=\"df-schema-field\"><strong>title: </strong> object</div>\n            \n              <div class=\"df-schema-field\"><strong>vec: </strong> object</div>\n            \n          </div>\n        </div>\n      </div>\n    </div>\n    \n    <!-- dataframe table -->\n    <div class=\"panel panel-default\">\n      \n      <div class=\"panel-heading\">\n        <h4 class=\"panel-title\" style=\"margin: 0px;\">\n          <a data-toggle=\"collapse\" href=\"#df-table-83dd298f\" data-parent=\"#df-table-wrapper-83dd298f\"> Table</a>\n        </h4>\n      </div>\n      \n      <div id=\"df-table-83dd298f\" class=\"panel-collapse collapse in\">\n        <div class=\"panel-body\">\n          \n          <input type=\"text\" class=\"df-table-search form-control input-sm\" placeholder=\"Search table\">\n          \n          <div>\n            \n            <span class=\"df-table-search-count\">Showing 19 of 19 rows</span>\n            \n          </div>\n          <!-- fixed header for when dataframe table scrolls -->\n          <div class=\"fixed-header-container\">\n            <div class=\"fixed-header\" style=\"width: 972px;\">\n              <div class=\"fixed-row\">\n                \n                <div class=\"fixed-cell\" style=\"width: 294px;\">title</div>\n                \n                <div class=\"fixed-cell\" style=\"width: 677px;\">vec</div>\n                \n              </div>\n            </div>\n          </div>\n          <div class=\"df-table-container\">\n            <table class=\"df-table\">\n              <thead>\n                <tr>\n                  \n                  <th><div>title</div></th>\n                  \n                  <th><div>vec</div></th>\n                  \n                </tr>\n              </thead>\n              <tbody>\n                \n                <tr>\n                  \n                  <td>julius_cesar</td>\n                  \n                  <td>[458, 460, 214, 531, 0, 272, 0, 695, 0, 241]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>king_lear</td>\n                  \n                  <td>[561, 496, 280, 705, 0, 332, 0, 876, 0, 364]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>othello</td>\n                  \n                  <td>[573, 596, 322, 874, 0, 401, 0, 888, 0, 432]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>emma</td>\n                  \n                  <td>[5402, 2319, 2172, 3200, 0, 1262, 0, 6348, 0, 3062]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>romeo_and_juliet</td>\n                  \n                  <td>[564, 428, 342, 655, 0, 345, 0, 955, 0, 462]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>persuasion</td>\n                  \n                  <td>[3083, 959, 1346, 1124, 0, 574, 0, 3815, 0, 1529]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>mansfield_park</td>\n                  \n                  <td>[5912, 2099, 2501, 2364, 0, 1140, 0, 7512, 0, 3065]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>northanger_abbey</td>\n                  \n                  <td>[2835, 1282, 1222, 1285, 0, 613, 0, 3629, 0, 1474]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>macbeth</td>\n                  \n                  <td>[454, 293, 198, 346, 0, 184, 0, 730, 0, 258]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>lady_susan</td>\n                  \n                  <td>[925, 462, 391, 804, 0, 337, 0, 994, 0, 365]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>merchant_of_venice</td>\n                  \n                  <td>[572, 548, 280, 676, 0, 330, 0, 1033, 0, 444]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>henry_V</td>\n                  \n                  <td>[841, 487, 428, 471, 0, 244, 0, 1230, 0, 459]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>richard_III</td>\n                  \n                  <td>[753, 525, 385, 753, 0, 455, 0, 1122, 0, 330]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>romeoandjuliet</td>\n                  \n                  <td>[564, 428, 342, 655, 0, 345, 0, 955, 0, 462]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>prideandpredjudice</td>\n                  \n                  <td>[4464, 1824, 1850, 2066, 0, 1122, 0, 5280, 0, 1962]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>midsummer</td>\n                  \n                  <td>[382, 367, 240, 457, 0, 256, 0, 704, 0, 279]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>hamlet</td>\n                  \n                  <td>[771, 668, 427, 617, 0, 377, 0, 1252, 0, 496]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>tempest</td>\n                  \n                  <td>[515, 366, 262, 559, 0, 250, 0, 829, 0, 355]</td>\n                  \n                </tr>\n                \n                <tr>\n                  \n                  <td>senseandsensibility</td>\n                  \n                  <td>[4495, 1844, 1959, 2004, 0, 1120, 0, 5277, 0, 2100]</td>\n                  \n                </tr>\n                \n              </tbody>\n            </table>\n          </div>\n        </div>\n      </div>\n    </div>\n  </div>\n  \n  <script class=\"pd_save\">\n    $(function() {\n      var tableWrapper = $('.df-table-wrapper-83dd298f');\n      var fixedHeader = $('.fixed-header', tableWrapper);\n      var tableContainer = $('.df-table-container', tableWrapper);\n      var table = $('.df-table', tableContainer);\n      var rows = $('tbody > tr', table);\n      var total = 19;\n  \n      fixedHeader\n        .css('width', table.width())\n        .find('.fixed-cell')\n        .each(function(i, e) {\n          $(this).css('width', $('.df-table-wrapper-83dd298f th:nth-child(' + (i+1) + ')').css('width'));\n        });\n  \n      tableContainer.scroll(function() {\n        fixedHeader.css({ left: table.position().left });\n      });\n  \n      rows.on(\"click\", function(e){\n          var txt = e.delegateTarget.innerText;\n          var splits = txt.split(\"\\t\");\n          var len = splits.length;\n          var hdrs = $(fixedHeader).find(\".fixed-cell\");\n          // Add all cells in the selected row as a map to be consumed by the target as needed\n          var payload = {type:\"select\", targetDivId: \"\" };\n          for (var i = 0; i < len; i++) {\n            payload[hdrs[i].innerHTML] = splits[i];\n          }\n  \n          //simple selection highlighting, client adds \"selected\" class\n          $(this).addClass(\"selected\").siblings().removeClass(\"selected\");\n          $(document).trigger('pd_event', payload);\n      });\n  \n      $('.df-table-search', tableWrapper).keyup(function() {\n        var val = '^(?=.*\\\\b' + $.trim($(this).val()).split(/\\s+/).join('\\\\b)(?=.*\\\\b') + ').*$';\n        var reg = RegExp(val, 'i');\n        var index = 0;\n        \n        rows.each(function(i, e) {\n          if (!reg.test($(this).text().replace(/\\s+/g, ' '))) {\n            $(this).attr('class', 'hidden');\n          }\n          else {\n            $(this).attr('class', (++index % 2 == 0 ? 'even' : 'odd'));\n          }\n        });\n        $('.df-table-search-count', tableWrapper).html('Showing ' + index + ' of ' + total + ' rows');\n      });\n    });\n  \n    $(\".df-table-wrapper td:contains('http://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n    $(\".df-table-wrapper td:contains('https://')\").each(function(){var tc = this.textContent; $(this).wrapInner(\"<a target='_blank' href='\" + tc + \"'></a>\");});\n  </script>\n  \n        </div>", 
                        "text/plain": "<IPython.core.display.HTML object>"
                    }
                }
            ]
        }
    ], 
    "nbformat_minor": 1, 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}